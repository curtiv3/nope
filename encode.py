#!/usr/bin/env python3
"""Re-encode edited PlayerPrefs settings back into localsave.bytes."""
from __future__ import annotations

import argparse
import base64
import json
import logging
import hashlib
import struct
import sys
from pathlib import Path
from typing import Any, Dict, Optional

try:
    import msgpack  # type: ignore
except ImportError as exc:  # pragma: no cover
    raise SystemExit(
        "msgpack is required. Install dependencies via 'pip install -r requirements.txt'."
    ) from exc

try:
    import zstandard  # type: ignore
except ImportError:  # pragma: no cover
    zstandard = None  # type: ignore

try:
    import lz4.block as lz4_block  # type: ignore
except ImportError:  # pragma: no cover
    lz4_block = None  # type: ignore

PRIMARY_XOR = 0xAA
LOGGER = logging.getLogger("encode")
DEBUG_DIR = Path("debug")


class EncodeError(RuntimeError):
    """Raised when encoding fails."""


def parse_args(argv: Optional[List[str]] = None) -> argparse.Namespace:
    parser = argparse.ArgumentParser(description=__doc__)
    parser.add_argument("input", type=Path, help="Path to settings.json or settings_raw.json")
    parser.add_argument("--out", type=Path, default=Path("localsave.bytes"), help="Output localsave path")
    parser.add_argument("--orig", type=Path, default=None, help="Path to original localsave backup (.bak)")
    parser.add_argument("--meta", type=Path, default=None, help="Path to metadata JSON generated by decode.py")
    parser.add_argument(
        "--compress",
        choices=["zstd", "lz4", "raw"],
        default=None,
        help="Compression algorithm to use. Defaults to metadata value or raw",
    )
    parser.add_argument("--slot", type=int, default=None, help="Slot identifier for deriving XOR key")
    parser.add_argument("--key", type=int, default=None, help="Explicit secondary XOR key override (0-255)")
    parser.add_argument("--dry-run", action="store_true", help="Perform encoding without writing output")
    parser.add_argument("--debug", action="store_true", help="Dump intermediate buffers to ./debug/")
    parser.add_argument("--verbose", action="store_true", help="Enable verbose logging")
    return parser.parse_args(argv)


def configure_logging(verbose: bool) -> None:
    logging.basicConfig(
        level=logging.DEBUG if verbose else logging.INFO,
        format="%(levelname)s:%(name)s:%(message)s",
    )


def write_debug(name: str, data: bytes, enabled: bool, dry_run: bool) -> None:
    if not enabled:
        return
    if dry_run:
        LOGGER.info("[dry-run] Skipping debug dump %s", name)
        return
    DEBUG_DIR.mkdir(exist_ok=True)
    target = DEBUG_DIR / name
    LOGGER.debug("Writing debug buffer %s (%d bytes)", target, len(data))
    target.write_bytes(data)


def load_json(path: Path) -> Any:
    try:
        return json.loads(path.read_text(encoding="utf-8"))
    except FileNotFoundError as exc:
        raise EncodeError(f"JSON file {path} not found") from exc


def load_metadata(meta_path: Path) -> Dict[str, Any]:
    try:
        return json.loads(meta_path.read_text(encoding="utf-8"))
    except FileNotFoundError as exc:
        raise EncodeError(
            f"Metadata file {meta_path} missing. Please run decode.py before encode.py."
        ) from exc


def resolve_header(meta: Dict[str, Any], orig_path: Optional[Path]) -> bytes:
    if "header_b64" in meta:
        try:
            return base64.b64decode(meta["header_b64"])
        except Exception as exc:
            raise EncodeError("Metadata header_b64 is invalid") from exc
    if orig_path and orig_path.exists():
        LOGGER.warning("Metadata missing header. Deriving from original backup directly.")
        data = orig_path.read_bytes()
        decrypted = bytes(b ^ PRIMARY_XOR for b in data)
        hdr_len = struct.unpack_from("<I", decrypted, 4)[0]
        payload_off = 8 + hdr_len
        return decrypted[:payload_off]
    raise EncodeError("No header information available")


def to_python_from_raw(node: Any) -> Any:
    if isinstance(node, dict) and "__type__" in node:
        t = node["__type__"]
        if t == "dict":
            result = {}
            for entry in node.get("entries", []):
                key = to_python_from_raw(entry["key"])
                value = to_python_from_raw(entry["value"])
                result[key] = value
            return result
        if t == "list":
            return [to_python_from_raw(item) for item in node.get("items", [])]
        if t == "bytes":
            return bytes.fromhex(node.get("hex", ""))
        if t == "str":
            return node.get("value", "")
        if t == "bool":
            return bool(node.get("value", False))
        if t == "nil":
            return None
        if t == "int":
            return int(node.get("value", 0))
        if t == "float":
            return float(node.get("value", 0.0))
        if t == "repr":
            return node.get("value")
        raise EncodeError(f"Unsupported raw type {t}")
    if isinstance(node, list):
        return [to_python_from_raw(item) for item in node]
    return node


def determine_secondary_key(args: argparse.Namespace, meta: Dict[str, Any]) -> Optional[int]:
    if args.key is not None:
        if not (0 <= args.key < 256):
            raise EncodeError("--key must be in range 0..255")
        LOGGER.info("Using explicit secondary XOR key: %d", args.key)
        return args.key
    if args.slot is not None:
        key = args.slot % 256
        LOGGER.info("Using slot-derived secondary XOR key: %d", key)
        return key
    key = meta.get("secondary_key")
    if key is not None:
        LOGGER.info("Using metadata secondary XOR key: %s", key)
        return int(key)
    LOGGER.info("No secondary XOR key will be applied")
    return None


def determine_compression(args: argparse.Namespace, meta: Dict[str, Any]) -> str:
    if args.compress is not None:
        LOGGER.info("Compression forced to %s via CLI", args.compress)
        return args.compress
    comp = meta.get("compression", "raw")
    LOGGER.info("Using compression from metadata: %s", comp)
    return comp


def pack_payload(data: Any) -> bytes:
    return msgpack.packb(data, use_bin_type=True)


def apply_secondary_xor(data: bytes, key: Optional[int]) -> bytes:
    if key is None:
        return data
    return bytes(b ^ key for b in data)


def compress_payload(data: bytes, mode: str) -> bytes:
    if mode == "raw":
        return data
    if mode == "zstd":
        if zstandard is None:
            raise EncodeError("zstandard module not installed")
        compressor = zstandard.ZstdCompressor()
        return compressor.compress(data)
    if mode == "lz4":
        if lz4_block is None:
            raise EncodeError("lz4 module not installed")
        return lz4_block.compress(data)  # type: ignore[attr-defined]
    raise EncodeError(f"Unknown compression mode {mode}")


def make_backup(out_path: Path, dry_run: bool) -> None:
    if not out_path.exists():
        return
    backup_path = out_path.with_suffix(out_path.suffix + ".bak2")
    if backup_path.exists():
        LOGGER.info("Backup .bak2 already exists at %s", backup_path)
        return
    if dry_run:
        LOGGER.info("[dry-run] Would create secondary backup at %s", backup_path)
        return
    import shutil

    LOGGER.info("Creating secondary backup %s", backup_path)
    shutil.copy2(out_path, backup_path)


def resolve_meta_path(args: argparse.Namespace) -> Path:
    if args.meta is not None:
        LOGGER.info("Using metadata path provided via --meta: %s", args.meta)
        return args.meta
    candidates = []
    for base in (args.orig, args.out, args.input):
        if base is None:
            continue
        base_path = Path(base)
        candidates.append(Path(str(base_path) + ".meta.json"))
        if base_path.suffix:
            candidates.append(base_path.with_suffix(base_path.suffix + ".meta.json"))
    for candidate in candidates:
        if candidate.exists():
            LOGGER.info("Resolved metadata path: %s", candidate)
            return candidate
    if candidates:
        LOGGER.warning(
            "Metadata file not found. Attempting to use %s (may need manual adjustment)",
            candidates[0],
        )
        return candidates[0]
    raise EncodeError("Unable to infer metadata path; specify --meta explicitly")


def encode_file(args: argparse.Namespace) -> bytes:
    json_data = load_json(args.input)
    if isinstance(json_data, dict) and json_data.get("__type__") in {
        "dict",
        "list",
        "bytes",
        "str",
        "bool",
        "nil",
        "int",
        "float",
        "repr",
    }:
        LOGGER.info("Detected raw JSON structure")
        payload_object = to_python_from_raw(json_data)
    else:
        payload_object = json_data
    packed = pack_payload(payload_object)
    write_debug("after_pack.msgpack", packed, args.debug, args.dry_run)

    meta_path = resolve_meta_path(args)
    meta = load_metadata(meta_path)

    header = resolve_header(meta, args.orig)
    compression = determine_compression(args, meta)
    compressed = compress_payload(packed, compression)
    write_debug("after_compress.bin", compressed, args.debug, args.dry_run)

    secondary_key = determine_secondary_key(args, meta)
    encrypted_payload = apply_secondary_xor(compressed, secondary_key)
    write_debug("after_key.bin", encrypted_payload, args.debug, args.dry_run)

    combined = header + encrypted_payload
    final_bytes = bytes(b ^ PRIMARY_XOR for b in combined)
    write_debug("final_clear.bin", combined, args.debug, args.dry_run)
    write_debug("final_encrypted.bin", final_bytes, args.debug, args.dry_run)

    if args.dry_run:
        LOGGER.info("[dry-run] Encoding complete (no files written)")
        return final_bytes

    make_backup(args.out, args.dry_run)
    args.out.write_bytes(final_bytes)
    LOGGER.info("Wrote %s (%d bytes)", args.out, len(final_bytes))

    if args.orig and args.orig.exists():
        orig_data = args.orig.read_bytes()
        new_hash = hashlib.sha256(final_bytes).hexdigest()
        orig_hash = hashlib.sha256(orig_data).hexdigest()
        if new_hash == orig_hash:
            LOGGER.info("Result matches original backup (sha256=%s)", new_hash)
        else:
            LOGGER.info(
                "Result differs from original backup (new=%s, original=%s)",
                new_hash,
                orig_hash,
            )
    return final_bytes


def main(argv: Optional[List[str]] = None) -> int:
    args = parse_args(argv)
    configure_logging(args.verbose)
    try:
        encode_file(args)
    except EncodeError as exc:
        LOGGER.error("%s", exc)
        return 2
    except Exception as exc:  # pragma: no cover
        LOGGER.exception("Unexpected failure: %s", exc)
        return 1
    return 0


if __name__ == "__main__":  # pragma: no cover
    sys.exit(main())
